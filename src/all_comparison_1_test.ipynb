{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ot4XR7gPFj2"
      },
      "source": [
        "## TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saV8u_FEPDf7"
      },
      "source": [
        "## SBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOlmjoV1D2-N",
        "outputId": "16307fdc-e10c-45c5-f653-06e7772a9c96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 00:35:00 - INFO - Starting embedding classification comparing CBOW, Skip-gram, TF-IDF and SBERT\n",
            "2025-05-08 00:35:00 - INFO - \n",
            "----------------------------------------------------------------------\n",
            "Processing CBOW embeddings\n",
            "----------------------------------------------------------------------\n",
            "2025-05-08 00:35:00 - INFO - Loading CBOW embeddings...\n",
            "/tmp/ipykernel_33240/3094228492.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(f\"{file_name}\")\n",
            "2025-05-08 00:35:00 - INFO - Successfully loaded cbow embeddings from PT file: (13000, 100)\n",
            "2025-05-08 00:35:00 - INFO - Loaded cbow dataframe with shape: (13000, 102)\n",
            "2025-05-08 00:35:00 - INFO - Data shapes: X_train=(10400, 100), y_train=(10400, 12)\n",
            "2025-05-08 00:35:00 - INFO - Classes: ['Priceperformance' 'Product' 'Speedeslimat' 'convenience'\n",
            " 'fiyatperformans' 'good -up' 'hızlıteslimat' 'iyipaketleme'\n",
            " 'kaliteliürün' 'uygunfiyat' 'your quality' 'ürüngüzel']\n",
            "2025-05-08 00:35:00 - INFO - \n",
            "==================================================\n",
            "Training SimpleNN with CBOW\n",
            "==================================================\n",
            "2025-05-08 00:35:01 - INFO - Epoch [1/20], Train Loss: 0.2287, Val Loss: 0.1765\n",
            "2025-05-08 00:35:01 - INFO - Train Metrics: Acc=0.3403, F1=0.4226\n",
            "2025-05-08 00:35:01 - INFO - Test Metrics: Acc=0.3465, F1=0.4334\n",
            "2025-05-08 00:35:02 - INFO - Epoch [5/20], Train Loss: 0.1496, Val Loss: 0.1449\n",
            "2025-05-08 00:35:02 - INFO - Train Metrics: Acc=0.4898, F1=0.6117\n",
            "2025-05-08 00:35:02 - INFO - Test Metrics: Acc=0.4835, F1=0.6043\n",
            "2025-05-08 00:35:05 - INFO - Epoch [10/20], Train Loss: 0.1387, Val Loss: 0.1359\n",
            "2025-05-08 00:35:05 - INFO - Train Metrics: Acc=0.5132, F1=0.6583\n",
            "2025-05-08 00:35:05 - INFO - Test Metrics: Acc=0.5081, F1=0.6493\n",
            "2025-05-08 00:35:07 - INFO - Epoch [15/20], Train Loss: 0.1323, Val Loss: 0.1306\n",
            "2025-05-08 00:35:07 - INFO - Train Metrics: Acc=0.5583, F1=0.6914\n",
            "2025-05-08 00:35:07 - INFO - Test Metrics: Acc=0.5415, F1=0.6779\n",
            "2025-05-08 00:35:08 - INFO - Epoch [20/20], Train Loss: 0.1278, Val Loss: 0.1264\n",
            "2025-05-08 00:35:08 - INFO - Train Metrics: Acc=0.5718, F1=0.7109\n",
            "2025-05-08 00:35:08 - INFO - Test Metrics: Acc=0.5562, F1=0.6978\n",
            "/tmp/ipykernel_33240/3094228492.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
            "2025-05-08 00:35:08 - INFO - \n",
            "Final Model Performance:\n",
            "2025-05-08 00:35:08 - INFO - Training Set Metrics:\n",
            "2025-05-08 00:35:08 - INFO - Accuracy: 0.5718\n",
            "2025-05-08 00:35:08 - INFO - Precision: 0.8225\n",
            "2025-05-08 00:35:08 - INFO - Recall: 0.6386\n",
            "2025-05-08 00:35:08 - INFO - F1: 0.7109\n",
            "2025-05-08 00:35:08 - INFO - \n",
            "Test Set Metrics:\n",
            "2025-05-08 00:35:08 - INFO - Accuracy: 0.5562\n",
            "2025-05-08 00:35:08 - INFO - Precision: 0.8043\n",
            "2025-05-08 00:35:08 - INFO - Recall: 0.6295\n",
            "2025-05-08 00:35:08 - INFO - F1: 0.6978\n",
            "2025-05-08 00:35:08 - INFO - Results for SimpleNN with CBOW:\n",
            "2025-05-08 00:35:08 - INFO - TRAINING SET:\n",
            "2025-05-08 00:35:08 - INFO - Accuracy: 0.5718\n",
            "2025-05-08 00:35:08 - INFO - Precision: 0.8225\n",
            "2025-05-08 00:35:08 - INFO - Recall: 0.6386\n",
            "2025-05-08 00:35:09 - INFO - F1: 0.7109\n",
            "2025-05-08 00:35:09 - INFO - \n",
            "TEST SET:\n",
            "2025-05-08 00:35:09 - INFO - Accuracy: 0.5562\n",
            "2025-05-08 00:35:09 - INFO - Precision: 0.8043\n",
            "2025-05-08 00:35:09 - INFO - Recall: 0.6295\n",
            "2025-05-08 00:35:09 - INFO - F1: 0.6978\n",
            "2025-05-08 00:35:09 - INFO - \n",
            "==================================================\n",
            "Training DeepNN with CBOW\n",
            "==================================================\n",
            "2025-05-08 00:35:10 - INFO - Epoch [1/20], Train Loss: 0.2409, Val Loss: 0.1728\n",
            "2025-05-08 00:35:10 - INFO - Train Metrics: Acc=0.4220, F1=0.4802\n",
            "2025-05-08 00:35:10 - INFO - Test Metrics: Acc=0.4169, F1=0.4802\n",
            "2025-05-08 00:35:15 - INFO - Epoch [5/20], Train Loss: 0.1535, Val Loss: 0.1404\n",
            "2025-05-08 00:35:15 - INFO - Train Metrics: Acc=0.5272, F1=0.6553\n",
            "2025-05-08 00:35:15 - INFO - Test Metrics: Acc=0.5127, F1=0.6443\n",
            "2025-05-08 00:35:19 - INFO - Epoch [10/20], Train Loss: 0.1403, Val Loss: 0.1301\n",
            "2025-05-08 00:35:19 - INFO - Train Metrics: Acc=0.5731, F1=0.6933\n",
            "2025-05-08 00:35:19 - INFO - Test Metrics: Acc=0.5538, F1=0.6756\n",
            "2025-05-08 00:35:25 - INFO - Epoch [15/20], Train Loss: 0.1334, Val Loss: 0.1228\n",
            "2025-05-08 00:35:25 - INFO - Train Metrics: Acc=0.6112, F1=0.7264\n",
            "2025-05-08 00:35:25 - INFO - Test Metrics: Acc=0.5819, F1=0.7060\n",
            "2025-05-08 00:35:31 - INFO - Epoch [20/20], Train Loss: 0.1292, Val Loss: 0.1226\n",
            "2025-05-08 00:35:31 - INFO - Train Metrics: Acc=0.6050, F1=0.7292\n",
            "2025-05-08 00:35:31 - INFO - Test Metrics: Acc=0.5800, F1=0.7143\n",
            "/tmp/ipykernel_33240/3094228492.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
            "2025-05-08 00:35:31 - INFO - \n",
            "Final Model Performance:\n",
            "2025-05-08 00:35:31 - INFO - Training Set Metrics:\n",
            "2025-05-08 00:35:31 - INFO - Accuracy: 0.6169\n",
            "2025-05-08 00:35:31 - INFO - Precision: 0.8248\n",
            "2025-05-08 00:35:31 - INFO - Recall: 0.6715\n",
            "2025-05-08 00:35:31 - INFO - F1: 0.7384\n",
            "2025-05-08 00:35:31 - INFO - \n",
            "Test Set Metrics:\n",
            "2025-05-08 00:35:31 - INFO - Accuracy: 0.5965\n",
            "2025-05-08 00:35:31 - INFO - Precision: 0.8053\n",
            "2025-05-08 00:35:31 - INFO - Recall: 0.6598\n",
            "2025-05-08 00:35:31 - INFO - F1: 0.7219\n",
            "2025-05-08 00:35:31 - INFO - Results for DeepNN with CBOW:\n",
            "2025-05-08 00:35:31 - INFO - TRAINING SET:\n",
            "2025-05-08 00:35:31 - INFO - Accuracy: 0.6169\n",
            "2025-05-08 00:35:31 - INFO - Precision: 0.8248\n",
            "2025-05-08 00:35:31 - INFO - Recall: 0.6715\n",
            "2025-05-08 00:35:31 - INFO - F1: 0.7384\n",
            "2025-05-08 00:35:31 - INFO - \n",
            "TEST SET:\n",
            "2025-05-08 00:35:31 - INFO - Accuracy: 0.5965\n",
            "2025-05-08 00:35:31 - INFO - Precision: 0.8053\n",
            "2025-05-08 00:35:31 - INFO - Recall: 0.6598\n",
            "2025-05-08 00:35:31 - INFO - F1: 0.7219\n",
            "2025-05-08 00:35:32 - INFO - \n",
            "----------------------------------------------------------------------\n",
            "Processing SKIPGRAM embeddings\n",
            "----------------------------------------------------------------------\n",
            "2025-05-08 00:35:32 - INFO - Loading SKIPGRAM embeddings...\n",
            "/tmp/ipykernel_33240/3094228492.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(f\"{file_name}\")\n",
            "2025-05-08 00:35:32 - INFO - Successfully loaded skipgram embeddings from PT file: (13000, 100)\n",
            "2025-05-08 00:35:32 - INFO - Loaded skipgram dataframe with shape: (13000, 102)\n",
            "2025-05-08 00:35:32 - INFO - Data shapes: X_train=(10400, 100), y_train=(10400, 12)\n",
            "2025-05-08 00:35:32 - INFO - Classes: ['Priceperformance' 'Product' 'Speedeslimat' 'convenience'\n",
            " 'fiyatperformans' 'good -up' 'hızlıteslimat' 'iyipaketleme'\n",
            " 'kaliteliürün' 'uygunfiyat' 'your quality' 'ürüngüzel']\n",
            "2025-05-08 00:35:32 - INFO - \n",
            "==================================================\n",
            "Training SimpleNN with SKIPGRAM\n",
            "==================================================\n",
            "2025-05-08 00:35:32 - INFO - Epoch [1/20], Train Loss: 0.2434, Val Loss: 0.1728\n",
            "2025-05-08 00:35:32 - INFO - Train Metrics: Acc=0.3919, F1=0.4707\n",
            "2025-05-08 00:35:32 - INFO - Test Metrics: Acc=0.3842, F1=0.4685\n",
            "2025-05-08 00:35:33 - INFO - Epoch [5/20], Train Loss: 0.1337, Val Loss: 0.1269\n",
            "2025-05-08 00:35:33 - INFO - Train Metrics: Acc=0.5648, F1=0.6979\n",
            "2025-05-08 00:35:33 - INFO - Test Metrics: Acc=0.5465, F1=0.6857\n",
            "2025-05-08 00:35:35 - INFO - Epoch [10/20], Train Loss: 0.1203, Val Loss: 0.1176\n",
            "2025-05-08 00:35:35 - INFO - Train Metrics: Acc=0.6030, F1=0.7339\n",
            "2025-05-08 00:35:35 - INFO - Test Metrics: Acc=0.5923, F1=0.7231\n",
            "2025-05-08 00:35:37 - INFO - Epoch [15/20], Train Loss: 0.1147, Val Loss: 0.1147\n",
            "2025-05-08 00:35:37 - INFO - Train Metrics: Acc=0.6122, F1=0.7470\n",
            "2025-05-08 00:35:37 - INFO - Test Metrics: Acc=0.5935, F1=0.7350\n",
            "2025-05-08 00:35:40 - INFO - Epoch [20/20], Train Loss: 0.1117, Val Loss: 0.1121\n",
            "2025-05-08 00:35:40 - INFO - Train Metrics: Acc=0.6316, F1=0.7628\n",
            "2025-05-08 00:35:40 - INFO - Test Metrics: Acc=0.6123, F1=0.7496\n",
            "/tmp/ipykernel_33240/3094228492.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
            "2025-05-08 00:35:40 - INFO - \n",
            "Final Model Performance:\n",
            "2025-05-08 00:35:40 - INFO - Training Set Metrics:\n",
            "2025-05-08 00:35:40 - INFO - Accuracy: 0.6316\n",
            "2025-05-08 00:35:40 - INFO - Precision: 0.8287\n",
            "2025-05-08 00:35:40 - INFO - Recall: 0.7082\n",
            "2025-05-08 00:35:40 - INFO - F1: 0.7628\n",
            "2025-05-08 00:35:40 - INFO - \n",
            "Test Set Metrics:\n",
            "2025-05-08 00:35:40 - INFO - Accuracy: 0.6123\n",
            "2025-05-08 00:35:40 - INFO - Precision: 0.8084\n",
            "2025-05-08 00:35:40 - INFO - Recall: 0.6998\n",
            "2025-05-08 00:35:40 - INFO - F1: 0.7496\n",
            "2025-05-08 00:35:40 - INFO - Results for SimpleNN with SKIPGRAM:\n",
            "2025-05-08 00:35:40 - INFO - TRAINING SET:\n",
            "2025-05-08 00:35:40 - INFO - Accuracy: 0.6316\n",
            "2025-05-08 00:35:40 - INFO - Precision: 0.8287\n",
            "2025-05-08 00:35:40 - INFO - Recall: 0.7082\n",
            "2025-05-08 00:35:40 - INFO - F1: 0.7628\n",
            "2025-05-08 00:35:40 - INFO - \n",
            "TEST SET:\n",
            "2025-05-08 00:35:40 - INFO - Accuracy: 0.6123\n",
            "2025-05-08 00:35:40 - INFO - Precision: 0.8084\n",
            "2025-05-08 00:35:40 - INFO - Recall: 0.6998\n",
            "2025-05-08 00:35:40 - INFO - F1: 0.7496\n",
            "2025-05-08 00:35:41 - INFO - \n",
            "==================================================\n",
            "Training DeepNN with SKIPGRAM\n",
            "==================================================\n",
            "2025-05-08 00:35:41 - INFO - Epoch [1/20], Train Loss: 0.2393, Val Loss: 0.1670\n",
            "2025-05-08 00:35:41 - INFO - Train Metrics: Acc=0.4621, F1=0.4993\n",
            "2025-05-08 00:35:41 - INFO - Test Metrics: Acc=0.4623, F1=0.5044\n",
            "2025-05-08 00:35:43 - INFO - Epoch [5/20], Train Loss: 0.1366, Val Loss: 0.1292\n",
            "2025-05-08 00:35:43 - INFO - Train Metrics: Acc=0.5868, F1=0.7014\n",
            "2025-05-08 00:35:43 - INFO - Test Metrics: Acc=0.5762, F1=0.6950\n",
            "2025-05-08 00:35:45 - INFO - Epoch [10/20], Train Loss: 0.1252, Val Loss: 0.1169\n",
            "2025-05-08 00:35:45 - INFO - Train Metrics: Acc=0.6362, F1=0.7528\n",
            "2025-05-08 00:35:45 - INFO - Test Metrics: Acc=0.6127, F1=0.7381\n",
            "2025-05-08 00:35:49 - INFO - Epoch [15/20], Train Loss: 0.1194, Val Loss: 0.1144\n",
            "2025-05-08 00:35:49 - INFO - Train Metrics: Acc=0.6421, F1=0.7630\n",
            "2025-05-08 00:35:49 - INFO - Test Metrics: Acc=0.6277, F1=0.7478\n",
            "2025-05-08 00:35:57 - INFO - Epoch [20/20], Train Loss: 0.1142, Val Loss: 0.1125\n",
            "2025-05-08 00:35:57 - INFO - Train Metrics: Acc=0.6538, F1=0.7687\n",
            "2025-05-08 00:35:57 - INFO - Test Metrics: Acc=0.6235, F1=0.7458\n",
            "/tmp/ipykernel_33240/3094228492.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
            "2025-05-08 00:35:57 - INFO - \n",
            "Final Model Performance:\n",
            "2025-05-08 00:35:57 - INFO - Training Set Metrics:\n",
            "2025-05-08 00:35:57 - INFO - Accuracy: 0.6641\n",
            "2025-05-08 00:35:57 - INFO - Precision: 0.8443\n",
            "2025-05-08 00:35:57 - INFO - Recall: 0.7262\n",
            "2025-05-08 00:35:57 - INFO - F1: 0.7773\n",
            "2025-05-08 00:35:57 - INFO - \n",
            "Test Set Metrics:\n",
            "2025-05-08 00:35:57 - INFO - Accuracy: 0.6246\n",
            "2025-05-08 00:35:57 - INFO - Precision: 0.8204\n",
            "2025-05-08 00:35:57 - INFO - Recall: 0.7004\n",
            "2025-05-08 00:35:57 - INFO - F1: 0.7523\n",
            "2025-05-08 00:35:57 - INFO - Results for DeepNN with SKIPGRAM:\n",
            "2025-05-08 00:35:57 - INFO - TRAINING SET:\n",
            "2025-05-08 00:35:57 - INFO - Accuracy: 0.6641\n",
            "2025-05-08 00:35:57 - INFO - Precision: 0.8443\n",
            "2025-05-08 00:35:57 - INFO - Recall: 0.7262\n",
            "2025-05-08 00:35:57 - INFO - F1: 0.7773\n",
            "2025-05-08 00:35:57 - INFO - \n",
            "TEST SET:\n",
            "2025-05-08 00:35:57 - INFO - Accuracy: 0.6246\n",
            "2025-05-08 00:35:57 - INFO - Precision: 0.8204\n",
            "2025-05-08 00:35:57 - INFO - Recall: 0.7004\n",
            "2025-05-08 00:35:57 - INFO - F1: 0.7523\n",
            "2025-05-08 00:35:58 - INFO - \n",
            "----------------------------------------------------------------------\n",
            "Processing TFIDF embeddings\n",
            "----------------------------------------------------------------------\n",
            "2025-05-08 00:35:58 - INFO - Loading TFIDF embeddings...\n",
            "/tmp/ipykernel_33240/3094228492.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(f\"{file_name}\")\n",
            "2025-05-08 00:35:58 - INFO - Successfully loaded tfidf embeddings from PT file: (13000, 4347)\n",
            "2025-05-08 00:35:58 - INFO - Loaded tfidf dataframe with shape: (13000, 4349)\n",
            "2025-05-08 00:35:58 - INFO - Data shapes: X_train=(10400, 4347), y_train=(10400, 12)\n",
            "2025-05-08 00:35:58 - INFO - Classes: ['Priceperformance' 'Product' 'Speedeslimat' 'convenience'\n",
            " 'fiyatperformans' 'good -up' 'hızlıteslimat' 'iyipaketleme'\n",
            " 'kaliteliürün' 'uygunfiyat' 'your quality' 'ürüngüzel']\n",
            "2025-05-08 00:35:58 - INFO - \n",
            "==================================================\n",
            "Training SimpleNN with TFIDF\n",
            "==================================================\n",
            "2025-05-08 00:35:59 - INFO - Epoch [1/20], Train Loss: 0.3086, Val Loss: 0.1867\n",
            "2025-05-08 00:35:59 - INFO - Train Metrics: Acc=0.2938, F1=0.4029\n",
            "2025-05-08 00:35:59 - INFO - Test Metrics: Acc=0.2858, F1=0.3966\n",
            "2025-05-08 00:36:03 - INFO - Epoch [5/20], Train Loss: 0.0901, Val Loss: 0.1043\n",
            "2025-05-08 00:36:03 - INFO - Train Metrics: Acc=0.7367, F1=0.8461\n",
            "2025-05-08 00:36:03 - INFO - Test Metrics: Acc=0.6342, F1=0.7687\n",
            "2025-05-08 00:36:11 - INFO - Epoch [10/20], Train Loss: 0.0690, Val Loss: 0.1062\n",
            "2025-05-08 00:36:11 - INFO - Train Metrics: Acc=0.8075, F1=0.8947\n",
            "2025-05-08 00:36:11 - INFO - Test Metrics: Acc=0.6327, F1=0.7770\n",
            "2025-05-08 00:36:11 - INFO - Early stopping at epoch 10\n",
            "/tmp/ipykernel_33240/3094228492.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
            "2025-05-08 00:36:11 - INFO - \n",
            "Final Model Performance:\n",
            "2025-05-08 00:36:11 - INFO - Training Set Metrics:\n",
            "2025-05-08 00:36:11 - INFO - Accuracy: 0.7670\n",
            "2025-05-08 00:36:11 - INFO - Precision: 0.9139\n",
            "2025-05-08 00:36:11 - INFO - Recall: 0.8327\n",
            "2025-05-08 00:36:11 - INFO - F1: 0.8697\n",
            "2025-05-08 00:36:11 - INFO - \n",
            "Test Set Metrics:\n",
            "2025-05-08 00:36:11 - INFO - Accuracy: 0.6358\n",
            "2025-05-08 00:36:11 - INFO - Precision: 0.8471\n",
            "2025-05-08 00:36:11 - INFO - Recall: 0.7198\n",
            "2025-05-08 00:36:11 - INFO - F1: 0.7757\n",
            "2025-05-08 00:36:11 - INFO - Results for SimpleNN with TFIDF:\n",
            "2025-05-08 00:36:11 - INFO - TRAINING SET:\n",
            "2025-05-08 00:36:11 - INFO - Accuracy: 0.7670\n",
            "2025-05-08 00:36:11 - INFO - Precision: 0.9139\n",
            "2025-05-08 00:36:11 - INFO - Recall: 0.8327\n",
            "2025-05-08 00:36:11 - INFO - F1: 0.8697\n",
            "2025-05-08 00:36:11 - INFO - \n",
            "TEST SET:\n",
            "2025-05-08 00:36:11 - INFO - Accuracy: 0.6358\n",
            "2025-05-08 00:36:11 - INFO - Precision: 0.8471\n",
            "2025-05-08 00:36:11 - INFO - Recall: 0.7198\n",
            "2025-05-08 00:36:11 - INFO - F1: 0.7757\n",
            "2025-05-08 00:36:11 - INFO - \n",
            "==================================================\n",
            "Training DeepNN with TFIDF\n",
            "==================================================\n",
            "2025-05-08 00:36:13 - INFO - Epoch [1/20], Train Loss: 0.2708, Val Loss: 0.1698\n",
            "2025-05-08 00:36:13 - INFO - Train Metrics: Acc=0.4575, F1=0.5100\n",
            "2025-05-08 00:36:13 - INFO - Test Metrics: Acc=0.4358, F1=0.4973\n",
            "2025-05-08 00:36:23 - INFO - Epoch [5/20], Train Loss: 0.0946, Val Loss: 0.1203\n",
            "2025-05-08 00:36:23 - INFO - Train Metrics: Acc=0.7681, F1=0.8507\n",
            "2025-05-08 00:36:23 - INFO - Test Metrics: Acc=0.6350, F1=0.7564\n",
            "2025-05-08 00:36:30 - INFO - Early stopping at epoch 7\n",
            "/tmp/ipykernel_33240/3094228492.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
            "2025-05-08 00:36:30 - INFO - \n",
            "Final Model Performance:\n",
            "2025-05-08 00:36:30 - INFO - Training Set Metrics:\n",
            "2025-05-08 00:36:30 - INFO - Accuracy: 0.7429\n",
            "2025-05-08 00:36:30 - INFO - Precision: 0.8664\n",
            "2025-05-08 00:36:30 - INFO - Recall: 0.7857\n",
            "2025-05-08 00:36:30 - INFO - F1: 0.8232\n",
            "2025-05-08 00:36:30 - INFO - \n",
            "Test Set Metrics:\n",
            "2025-05-08 00:36:30 - INFO - Accuracy: 0.6215\n",
            "2025-05-08 00:36:30 - INFO - Precision: 0.7825\n",
            "2025-05-08 00:36:30 - INFO - Recall: 0.6951\n",
            "2025-05-08 00:36:30 - INFO - F1: 0.7349\n",
            "2025-05-08 00:36:30 - INFO - Results for DeepNN with TFIDF:\n",
            "2025-05-08 00:36:30 - INFO - TRAINING SET:\n",
            "2025-05-08 00:36:30 - INFO - Accuracy: 0.7429\n",
            "2025-05-08 00:36:30 - INFO - Precision: 0.8664\n",
            "2025-05-08 00:36:30 - INFO - Recall: 0.7857\n",
            "2025-05-08 00:36:30 - INFO - F1: 0.8232\n",
            "2025-05-08 00:36:30 - INFO - \n",
            "TEST SET:\n",
            "2025-05-08 00:36:30 - INFO - Accuracy: 0.6215\n",
            "2025-05-08 00:36:30 - INFO - Precision: 0.7825\n",
            "2025-05-08 00:36:30 - INFO - Recall: 0.6951\n",
            "2025-05-08 00:36:30 - INFO - F1: 0.7349\n",
            "2025-05-08 00:36:31 - INFO - \n",
            "----------------------------------------------------------------------\n",
            "Processing SBERT embeddings\n",
            "----------------------------------------------------------------------\n",
            "2025-05-08 00:36:31 - INFO - Loading SBERT embeddings...\n",
            "/tmp/ipykernel_33240/3094228492.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(f\"{file_name}\")\n",
            "2025-05-08 00:36:31 - INFO - Successfully loaded sbert embeddings from PT file: (13000, 384)\n",
            "2025-05-08 00:36:31 - INFO - Loaded sbert dataframe with shape: (13000, 386)\n",
            "2025-05-08 00:36:31 - INFO - Data shapes: X_train=(10400, 384), y_train=(10400, 12)\n",
            "2025-05-08 00:36:31 - INFO - Classes: ['Priceperformance' 'Product' 'Speedeslimat' 'convenience'\n",
            " 'fiyatperformans' 'good -up' 'hızlıteslimat' 'iyipaketleme'\n",
            " 'kaliteliürün' 'uygunfiyat' 'your quality' 'ürüngüzel']\n",
            "2025-05-08 00:36:31 - INFO - \n",
            "==================================================\n",
            "Training SimpleNN with SBERT\n",
            "==================================================\n",
            "2025-05-08 00:36:31 - INFO - Epoch [1/20], Train Loss: 0.2065, Val Loss: 0.0918\n",
            "2025-05-08 00:36:31 - INFO - Train Metrics: Acc=0.7116, F1=0.7919\n",
            "2025-05-08 00:36:31 - INFO - Test Metrics: Acc=0.6865, F1=0.7780\n",
            "2025-05-08 00:36:34 - INFO - Epoch [5/20], Train Loss: 0.0490, Val Loss: 0.0445\n",
            "2025-05-08 00:36:34 - INFO - Train Metrics: Acc=0.8627, F1=0.9224\n",
            "2025-05-08 00:36:34 - INFO - Test Metrics: Acc=0.8562, F1=0.9200\n",
            "2025-05-08 00:36:37 - INFO - Epoch [10/20], Train Loss: 0.0441, Val Loss: 0.0424\n",
            "2025-05-08 00:36:37 - INFO - Train Metrics: Acc=0.8684, F1=0.9271\n",
            "2025-05-08 00:36:37 - INFO - Test Metrics: Acc=0.8604, F1=0.9234\n",
            "2025-05-08 00:36:40 - INFO - Epoch [15/20], Train Loss: 0.0428, Val Loss: 0.0419\n",
            "2025-05-08 00:36:40 - INFO - Train Metrics: Acc=0.8730, F1=0.9306\n",
            "2025-05-08 00:36:40 - INFO - Test Metrics: Acc=0.8627, F1=0.9265\n",
            "2025-05-08 00:36:42 - INFO - Epoch [20/20], Train Loss: 0.0415, Val Loss: 0.0414\n",
            "2025-05-08 00:36:42 - INFO - Train Metrics: Acc=0.8754, F1=0.9326\n",
            "2025-05-08 00:36:42 - INFO - Test Metrics: Acc=0.8662, F1=0.9278\n",
            "/tmp/ipykernel_33240/3094228492.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
            "2025-05-08 00:36:42 - INFO - \n",
            "Final Model Performance:\n",
            "2025-05-08 00:36:42 - INFO - Training Set Metrics:\n",
            "2025-05-08 00:36:42 - INFO - Accuracy: 0.8754\n",
            "2025-05-08 00:36:42 - INFO - Precision: 0.9382\n",
            "2025-05-08 00:36:42 - INFO - Recall: 0.9288\n",
            "2025-05-08 00:36:42 - INFO - F1: 0.9326\n",
            "2025-05-08 00:36:42 - INFO - \n",
            "Test Set Metrics:\n",
            "2025-05-08 00:36:42 - INFO - Accuracy: 0.8662\n",
            "2025-05-08 00:36:42 - INFO - Precision: 0.9284\n",
            "2025-05-08 00:36:42 - INFO - Recall: 0.9275\n",
            "2025-05-08 00:36:42 - INFO - F1: 0.9278\n",
            "2025-05-08 00:36:42 - INFO - Results for SimpleNN with SBERT:\n",
            "2025-05-08 00:36:42 - INFO - TRAINING SET:\n",
            "2025-05-08 00:36:42 - INFO - Accuracy: 0.8754\n",
            "2025-05-08 00:36:42 - INFO - Precision: 0.9382\n",
            "2025-05-08 00:36:42 - INFO - Recall: 0.9288\n",
            "2025-05-08 00:36:42 - INFO - F1: 0.9326\n",
            "2025-05-08 00:36:42 - INFO - \n",
            "TEST SET:\n",
            "2025-05-08 00:36:42 - INFO - Accuracy: 0.8662\n",
            "2025-05-08 00:36:42 - INFO - Precision: 0.9284\n",
            "2025-05-08 00:36:42 - INFO - Recall: 0.9275\n",
            "2025-05-08 00:36:42 - INFO - F1: 0.9278\n",
            "2025-05-08 00:36:43 - INFO - \n",
            "==================================================\n",
            "Training DeepNN with SBERT\n",
            "==================================================\n",
            "2025-05-08 00:36:44 - INFO - Epoch [1/20], Train Loss: 0.2010, Val Loss: 0.0865\n",
            "2025-05-08 00:36:44 - INFO - Train Metrics: Acc=0.7150, F1=0.7752\n",
            "2025-05-08 00:36:44 - INFO - Test Metrics: Acc=0.6981, F1=0.7699\n",
            "2025-05-08 00:36:49 - INFO - Epoch [5/20], Train Loss: 0.0570, Val Loss: 0.0470\n",
            "2025-05-08 00:36:49 - INFO - Train Metrics: Acc=0.8675, F1=0.9225\n",
            "2025-05-08 00:36:49 - INFO - Test Metrics: Acc=0.8504, F1=0.9136\n",
            "2025-05-08 00:36:56 - INFO - Epoch [10/20], Train Loss: 0.0516, Val Loss: 0.0441\n",
            "2025-05-08 00:36:56 - INFO - Train Metrics: Acc=0.8703, F1=0.9278\n",
            "2025-05-08 00:36:56 - INFO - Test Metrics: Acc=0.8612, F1=0.9239\n",
            "2025-05-08 00:37:02 - INFO - Early stopping at epoch 13\n",
            "/tmp/ipykernel_33240/3094228492.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
            "2025-05-08 00:37:02 - INFO - \n",
            "Final Model Performance:\n",
            "2025-05-08 00:37:02 - INFO - Training Set Metrics:\n",
            "2025-05-08 00:37:02 - INFO - Accuracy: 0.8703\n",
            "2025-05-08 00:37:02 - INFO - Precision: 0.9358\n",
            "2025-05-08 00:37:02 - INFO - Recall: 0.9206\n",
            "2025-05-08 00:37:02 - INFO - F1: 0.9278\n",
            "2025-05-08 00:37:02 - INFO - \n",
            "Test Set Metrics:\n",
            "2025-05-08 00:37:02 - INFO - Accuracy: 0.8612\n",
            "2025-05-08 00:37:02 - INFO - Precision: 0.9275\n",
            "2025-05-08 00:37:02 - INFO - Recall: 0.9210\n",
            "2025-05-08 00:37:02 - INFO - F1: 0.9239\n",
            "2025-05-08 00:37:02 - INFO - Results for DeepNN with SBERT:\n",
            "2025-05-08 00:37:02 - INFO - TRAINING SET:\n",
            "2025-05-08 00:37:02 - INFO - Accuracy: 0.8703\n",
            "2025-05-08 00:37:02 - INFO - Precision: 0.9358\n",
            "2025-05-08 00:37:02 - INFO - Recall: 0.9206\n",
            "2025-05-08 00:37:02 - INFO - F1: 0.9278\n",
            "2025-05-08 00:37:02 - INFO - \n",
            "TEST SET:\n",
            "2025-05-08 00:37:02 - INFO - Accuracy: 0.8612\n",
            "2025-05-08 00:37:02 - INFO - Precision: 0.9275\n",
            "2025-05-08 00:37:02 - INFO - Recall: 0.9210\n",
            "2025-05-08 00:37:02 - INFO - F1: 0.9239\n",
            "2025-05-08 00:37:05 - INFO - Classification comparison completed! Results saved to 'results' directory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification results summary:\n",
            "Embedding    Model      Set Accuracy Precision Recall     F1\n",
            "     CBOW SimpleNN Training   0.5718    0.8225 0.6386 0.7109\n",
            "     CBOW   DeepNN Training   0.6169    0.8248 0.6715 0.7384\n",
            " SKIPGRAM SimpleNN Training   0.6316    0.8287 0.7082 0.7628\n",
            " SKIPGRAM   DeepNN Training   0.6641    0.8443 0.7262 0.7773\n",
            "    TFIDF SimpleNN Training   0.7670    0.9139 0.8327 0.8697\n",
            "    TFIDF   DeepNN Training   0.7429    0.8664 0.7857 0.8232\n",
            "    SBERT SimpleNN Training   0.8754    0.9382 0.9288 0.9326\n",
            "    SBERT   DeepNN Training   0.8703    0.9358 0.9206 0.9278\n",
            "     CBOW SimpleNN     Test   0.5562    0.8043 0.6295 0.6978\n",
            "     CBOW   DeepNN     Test   0.5965    0.8053 0.6598 0.7219\n",
            " SKIPGRAM SimpleNN     Test   0.6123    0.8084 0.6998 0.7496\n",
            " SKIPGRAM   DeepNN     Test   0.6246    0.8204 0.7004 0.7523\n",
            "    TFIDF SimpleNN     Test   0.6358    0.8471 0.7198 0.7757\n",
            "    TFIDF   DeepNN     Test   0.6215    0.7825 0.6951 0.7349\n",
            "    SBERT SimpleNN     Test   0.8662    0.9284 0.9275 0.9278\n",
            "    SBERT   DeepNN     Test   0.8612    0.9275 0.9210 0.9239\n",
            "\n",
            "Average performance by embedding type:\n",
            "\n",
            "Training Set:\n",
            "CBOW: Accuracy: 0.5944, Precision: 0.8236, Recall: 0.6550, F1: 0.7247\n",
            "SKIPGRAM: Accuracy: 0.6479, Precision: 0.8365, Recall: 0.7172, F1: 0.7701\n",
            "TFIDF: Accuracy: 0.7550, Precision: 0.8902, Recall: 0.8092, F1: 0.8465\n",
            "SBERT: Accuracy: 0.8728, Precision: 0.9370, Recall: 0.9247, F1: 0.9302\n",
            "\n",
            "Test Set:\n",
            "CBOW: Accuracy: 0.5763, Precision: 0.8048, Recall: 0.6446, F1: 0.7098\n",
            "SKIPGRAM: Accuracy: 0.6185, Precision: 0.8144, Recall: 0.7001, F1: 0.7510\n",
            "TFIDF: Accuracy: 0.6287, Precision: 0.8148, Recall: 0.7074, F1: 0.7553\n",
            "SBERT: Accuracy: 0.8637, Precision: 0.9280, Recall: 0.9242, F1: 0.9258\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import ast\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "    level=logging.INFO\n",
        ")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# === 1. Helper Functions ===\n",
        "def safe_parse(lst_str):\n",
        "    \"\"\"Safely parse string representations of lists\"\"\"\n",
        "    try:\n",
        "        if isinstance(lst_str, list):\n",
        "            return lst_str\n",
        "        return ast.literal_eval(lst_str)\n",
        "    except:\n",
        "        return [lst_str]\n",
        "\n",
        "# === 2. Load Embeddings ===\n",
        "def load_embeddings(embedding_type):\n",
        "    \"\"\"Load document embeddings from either PT or CSV files\n",
        "\n",
        "    Args:\n",
        "        embedding_type: 'cbow', 'skipgram', 'tfidf', or 'sbert'\n",
        "    \"\"\"\n",
        "    logging.info(f\"Loading {embedding_type.upper()} embeddings...\")\n",
        "\n",
        "    try:\n",
        "        # First try to load from PT file\n",
        "        if embedding_type == 'tfidf':\n",
        "            file_name = \"tfidf_1.pt\"\n",
        "        elif embedding_type == 'sbert':\n",
        "            file_name = f\"{embedding_type}_output_1.pt\"\n",
        "        else:\n",
        "            file_name = f\"{embedding_type}_1.pt\"\n",
        "        data = torch.load(f\"{file_name}\")\n",
        "        embeddings = data['embeddings'].numpy()\n",
        "        labels = data['labels']\n",
        "        indices = data['indices']\n",
        "\n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(embeddings)\n",
        "        df.insert(0, 'original_index', indices)\n",
        "        df['labels'] = labels\n",
        "        logging.info(f\"Successfully loaded {embedding_type} embeddings from PT file: {embeddings.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Fallback to CSV file\n",
        "        logging.info(f\"Failed to load from PT file: {e}\")\n",
        "        logging.info(\"Trying to load from CSV file...\")\n",
        "\n",
        "        if embedding_type == 'tfidf':\n",
        "            file_name = \"tfidf_1.csv\"\n",
        "        elif embedding_type == \"sbert\":\n",
        "            return\n",
        "        else:\n",
        "            return\n",
        "            # file_name = f\"{embedding_type}_1.csv\"\n",
        "\n",
        "        csv_path = f\"{file_name}\"\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # For TF-IDF, check if we need to rename the label column\n",
        "        if embedding_type == 'tfidf' and 'CommentClass_en' in df.columns:\n",
        "            df.rename(columns={'CommentClass_en': 'labels'}, inplace=True)\n",
        "\n",
        "        logging.info(f\"Successfully loaded {embedding_type} embeddings from CSV: {csv_path}\")\n",
        "\n",
        "    # Ensure labels are in the correct format\n",
        "    if 'labels' in df.columns:\n",
        "        df['labels'] = df['labels'].apply(safe_parse)\n",
        "    else:\n",
        "        # Try to find alternative label column\n",
        "        label_candidates = ['CommentClass_en', 'label', 'classes', 'class']\n",
        "        for col in label_candidates:\n",
        "            if col in df.columns:\n",
        "                df.rename(columns={col: 'labels'}, inplace=True)\n",
        "                df['labels'] = df['labels'].apply(safe_parse)\n",
        "                break\n",
        "        else:\n",
        "            logging.error(f\"No label column found in {embedding_type} embeddings\")\n",
        "            raise ValueError(f\"No label column found in {embedding_type} embeddings\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# === 3. Neural Network Models ===\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    \"\"\"Simple Feed-Forward Neural Network for multi-label classification\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class DeepNN(nn.Module):\n",
        "    \"\"\"Deep Neural Network with multiple hidden layers\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.4):\n",
        "        super(DeepNN, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Add hidden layers\n",
        "        for dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev_dim = dim\n",
        "\n",
        "        # Add output layer with sigmoid for multi-label\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def prepare_data(df, test_size=0.2, random_state=42):\n",
        "    \"\"\"Prepare data for training and evaluation\"\"\"\n",
        "    # Extract features and labels\n",
        "    feature_cols = [col for col in df.columns if col not in ['original_index', 'labels', 'Unnamed: 0']]\n",
        "    X = df[feature_cols].values\n",
        "\n",
        "    # Process labels\n",
        "    y_raw = df['labels'].tolist()\n",
        "\n",
        "    # Binarize labels\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    y = mlb.fit_transform(y_raw)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=None\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Data shapes: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "    logging.info(f\"Classes: {mlb.classes_}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, mlb.classes_\n",
        "\n",
        "\n",
        "def train_model(model, X_train, y_train, X_test, y_test, model_name, embedding_type,\n",
        "                batch_size=32, lr=0.001, num_epochs=20, patience=3):\n",
        "    \"\"\"Train and evaluate a PyTorch model with metrics for both training and test sets\"\"\"\n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    y_train_tensor = torch.FloatTensor(y_train)\n",
        "    X_test_tensor = torch.FloatTensor(X_test)\n",
        "    y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # For early stopping\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Lists to store metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    # Lists to store performance metrics per epoch\n",
        "    train_metrics_history = []\n",
        "    test_metrics_history = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Evaluate on both training and test sets\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Training set predictions\n",
        "            train_outputs = model(X_train_tensor)\n",
        "            train_loss = criterion(train_outputs, y_train_tensor)\n",
        "            train_pred_binary = (train_outputs > 0.5).float().numpy()\n",
        "            \n",
        "            # Test set predictions\n",
        "            test_outputs = model(X_test_tensor)\n",
        "            test_loss = criterion(test_outputs, y_test_tensor)\n",
        "            test_pred_binary = (test_outputs > 0.5).float().numpy()\n",
        "            \n",
        "            val_losses.append(test_loss.item())\n",
        "            \n",
        "            # Calculate metrics for training set\n",
        "            train_metrics = {\n",
        "                'accuracy': accuracy_score(y_train, train_pred_binary),\n",
        "                'precision': precision_score(y_train, train_pred_binary, average='weighted', zero_division=0),\n",
        "                'recall': recall_score(y_train, train_pred_binary, average='weighted', zero_division=0),\n",
        "                'f1': f1_score(y_train, train_pred_binary, average='weighted', zero_division=0)\n",
        "            }\n",
        "            train_metrics_history.append(train_metrics)\n",
        "            \n",
        "            # Calculate metrics for test set\n",
        "            test_metrics = {\n",
        "                'accuracy': accuracy_score(y_test, test_pred_binary),\n",
        "                'precision': precision_score(y_test, test_pred_binary, average='weighted', zero_division=0),\n",
        "                'recall': recall_score(y_test, test_pred_binary, average='weighted', zero_division=0),\n",
        "                'f1': f1_score(y_test, test_pred_binary, average='weighted', zero_division=0)\n",
        "            }\n",
        "            test_metrics_history.append(test_metrics)\n",
        "\n",
        "            # Early stopping check\n",
        "            if test_loss < best_loss:\n",
        "                best_loss = test_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), f\"best_{embedding_type}_{model_name}_classifier.pt\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            logging.info(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "                       f'Train Loss: {avg_train_loss:.4f}, '\n",
        "                       f'Val Loss: {test_loss:.4f}')\n",
        "            logging.info(f'Train Metrics: Acc={train_metrics[\"accuracy\"]:.4f}, '\n",
        "                       f'F1={train_metrics[\"f1\"]:.4f}')\n",
        "            logging.info(f'Test Metrics: Acc={test_metrics[\"accuracy\"]:.4f}, '\n",
        "                       f'F1={test_metrics[\"f1\"]:.4f}')\n",
        "\n",
        "        # Check if early stopping criteria is met\n",
        "        if patience_counter >= patience:\n",
        "            logging.info(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model for evaluation\n",
        "    model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
        "\n",
        "    # Final evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get predictions for both train and test sets\n",
        "        train_outputs = model(X_train_tensor)\n",
        "        train_pred_binary = (train_outputs > 0.5).float().numpy()\n",
        "        \n",
        "        test_outputs = model(X_test_tensor)\n",
        "        test_pred_binary = (test_outputs > 0.5).float().numpy()\n",
        "    \n",
        "    # Calculate final metrics for training set\n",
        "    train_accuracy = accuracy_score(y_train, train_pred_binary)\n",
        "    train_precision = precision_score(y_train, train_pred_binary, average='weighted', zero_division=0)\n",
        "    train_recall = recall_score(y_train, train_pred_binary, average='weighted', zero_division=0)\n",
        "    train_f1 = f1_score(y_train, train_pred_binary, average='weighted', zero_division=0)\n",
        "    \n",
        "    train_metrics = {\n",
        "        'accuracy': train_accuracy,\n",
        "        'precision': train_precision,\n",
        "        'recall': train_recall,\n",
        "        'f1': train_f1,\n",
        "    }\n",
        "    \n",
        "    # Calculate final metrics for test set\n",
        "    test_accuracy = accuracy_score(y_test, test_pred_binary)\n",
        "    test_precision = precision_score(y_test, test_pred_binary, average='weighted', zero_division=0)\n",
        "    test_recall = recall_score(y_test, test_pred_binary, average='weighted', zero_division=0)\n",
        "    test_f1 = f1_score(y_test, test_pred_binary, average='weighted', zero_division=0)\n",
        "    \n",
        "    test_metrics = {\n",
        "        'accuracy': test_accuracy,\n",
        "        'precision': test_precision,\n",
        "        'recall': test_recall,\n",
        "        'f1': test_f1,\n",
        "    }\n",
        "    \n",
        "    # Log final results\n",
        "    logging.info(\"\\nFinal Model Performance:\")\n",
        "    logging.info(\"Training Set Metrics:\")\n",
        "    for metric, value in train_metrics.items():\n",
        "        logging.info(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "    \n",
        "    logging.info(\"\\nTest Set Metrics:\")\n",
        "    for metric, value in test_metrics.items():\n",
        "        logging.info(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "\n",
        "    # Get per-class metrics for both sets\n",
        "    train_class_report = classification_report(y_train, train_pred_binary,\n",
        "                                         zero_division=0, output_dict=True)\n",
        "    test_class_report = classification_report(y_test, test_pred_binary,\n",
        "                                        zero_division=0, output_dict=True)\n",
        "\n",
        "    return model, train_metrics, test_metrics, train_losses, val_losses, train_metrics_history, test_metrics_history, train_class_report, test_class_report\n",
        "\n",
        "\n",
        "def plot_training_curves(train_losses, val_losses, train_metrics_history, test_metrics_history, model_name, embedding_type):\n",
        "    \"\"\"Plot training and validation loss curves along with performance metrics\"\"\"\n",
        "    # Plot loss curves\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Plot 1: Losses\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title(f'Loss Curves for {model_name} ({embedding_type.upper()})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Plot 2: Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    train_acc = [metrics['accuracy'] for metrics in train_metrics_history]\n",
        "    test_acc = [metrics['accuracy'] for metrics in test_metrics_history]\n",
        "    plt.plot(train_acc, label='Training Accuracy')\n",
        "    plt.plot(test_acc, label='Test Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Plot 3: Precision and Recall (Training)\n",
        "    plt.subplot(2, 2, 3)\n",
        "    train_precision = [metrics['precision'] for metrics in train_metrics_history]\n",
        "    train_recall = [metrics['recall'] for metrics in train_metrics_history]\n",
        "    plt.plot(train_precision, label='Training Precision')\n",
        "    plt.plot(train_recall, label='Training Recall')\n",
        "    plt.title('Training Set: Precision and Recall')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Plot 4: Precision and Recall (Test)\n",
        "    plt.subplot(2, 2, 4)\n",
        "    test_precision = [metrics['precision'] for metrics in test_metrics_history]\n",
        "    test_recall = [metrics['recall'] for metrics in test_metrics_history]\n",
        "    plt.plot(test_precision, label='Test Precision')\n",
        "    plt.plot(test_recall, label='Test Recall')\n",
        "    plt.title('Test Set: Precision and Recall')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{embedding_type}_{model_name}_metrics.png\")\n",
        "    plt.close()\n",
        "    \n",
        "    # Create an additional plot for F1 scores\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    train_f1 = [metrics['f1'] for metrics in train_metrics_history]\n",
        "    test_f1 = [metrics['f1'] for metrics in test_metrics_history]\n",
        "    plt.plot(train_f1, label='Training F1')\n",
        "    plt.plot(test_f1, label='Test F1')\n",
        "    plt.title(f'F1 Score Evolution for {model_name} ({embedding_type.upper()})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{embedding_type}_{model_name}_f1_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def process_embedding_type(embedding_type):\n",
        "    \"\"\"Process a specific embedding type (CBOW, Skip-gram, TF-IDF, or SBERT)\"\"\"\n",
        "    logging.info(f\"\\n{'-'*70}\\nProcessing {embedding_type.upper()} embeddings\\n{'-'*70}\")\n",
        "\n",
        "    # Load embeddings\n",
        "    df = load_embeddings(embedding_type)\n",
        "    logging.info(f\"Loaded {embedding_type} dataframe with shape: {df.shape}\")\n",
        "\n",
        "    # Prepare data\n",
        "    X_train, X_test, y_train, y_test, classes = prepare_data(df)\n",
        "\n",
        "    # Model parameters\n",
        "    input_dim = X_train.shape[1]\n",
        "    output_dim = y_train.shape[1]\n",
        "\n",
        "    # Define models to test\n",
        "    models = [\n",
        "        {\n",
        "            'name': 'SimpleNN',\n",
        "            'model': SimpleNN(input_dim, hidden_dim=128, output_dim=output_dim)\n",
        "        },\n",
        "        {\n",
        "            'name': 'DeepNN',\n",
        "            'model': DeepNN(\n",
        "                input_dim,\n",
        "                hidden_dims=[256, 128, 64],\n",
        "                output_dim=output_dim\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    results = {}\n",
        "\n",
        "    for model_info in models:\n",
        "        name = model_info['name']\n",
        "        model = model_info['model']\n",
        "\n",
        "        logging.info(f\"\\n{'='*50}\\nTraining {name} with {embedding_type.upper()}\\n{'='*50}\")\n",
        "\n",
        "        trained_model, train_metrics, test_metrics, train_losses, val_losses, train_metrics_history, test_metrics_history, train_class_report, test_class_report = train_model(\n",
        "            model, X_train, y_train, X_test, y_test, name, embedding_type\n",
        "        )\n",
        "\n",
        "        # Log results\n",
        "        logging.info(f\"Results for {name} with {embedding_type.upper()}:\")\n",
        "        logging.info(\"TRAINING SET:\")\n",
        "        for metric, value in train_metrics.items():\n",
        "            logging.info(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "            \n",
        "        logging.info(\"\\nTEST SET:\")\n",
        "        for metric, value in test_metrics.items():\n",
        "            logging.info(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "\n",
        "        # Plot learning curves with metrics\n",
        "        plot_training_curves(train_losses, val_losses, train_metrics_history, test_metrics_history, name, embedding_type)\n",
        "\n",
        "        # Save detailed class reports\n",
        "        pd.DataFrame(train_class_report).transpose().to_csv(\n",
        "            f\"{embedding_type}_{name}_train_class_report.csv\"\n",
        "        )\n",
        "        pd.DataFrame(test_class_report).transpose().to_csv(\n",
        "            f\"{embedding_type}_{name}_test_class_report.csv\"\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        torch.save({\n",
        "            'model_state_dict': trained_model.state_dict(),\n",
        "            'class_names': classes,\n",
        "            'train_metrics': train_metrics,\n",
        "            'test_metrics': test_metrics,\n",
        "            'architecture': str(trained_model)\n",
        "        }, f\"{embedding_type}_{name}_classifier.pt\")\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'train': train_metrics,\n",
        "            'test': test_metrics\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_metrics_comparison(results_dict):\n",
        "    \"\"\"Plot performance metrics comparison between models and embedding types\"\"\"\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    data = []\n",
        "    for embedding_type, models_data in results_dict.items():\n",
        "        for model_name, metrics_dict in models_data.items():\n",
        "            model_label = f\"{model_name} ({embedding_type.upper()})\"\n",
        "            for metric in metrics:\n",
        "                data.append({\n",
        "                    'Model': model_label,\n",
        "                    'Metric': metric.capitalize(),\n",
        "                    'Value': metrics_dict[metric]\n",
        "                })\n",
        "\n",
        "    df_plot = pd.DataFrame(data)\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    sns.barplot(x='Model', y='Value', hue='Metric', data=df_plot)\n",
        "    plt.title('Performance Metrics Comparison: CBOW vs Skip-gram vs TF-IDF vs SBERT')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"embedding_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_embedding_comparison(results_dict):\n",
        "    \"\"\"Plot comparison of embedding types across metrics and models\"\"\"\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    model_names = list(next(iter(results_dict.values())).keys())\n",
        "\n",
        "    # For each metric, compare embeddings across models\n",
        "    for metric in metrics:\n",
        "        plt.figure(figsize=(12, 7))\n",
        "\n",
        "        # Prepare data\n",
        "        x = np.arange(len(model_names))\n",
        "        width = 0.2  # Width of the bars\n",
        "\n",
        "        # Create the bars for each embedding type\n",
        "        embedding_types = list(results_dict.keys())\n",
        "        for i, emb_type in enumerate(embedding_types):\n",
        "            values = [results_dict[emb_type][model][metric] for model in model_names]\n",
        "            offset = width * (i - len(embedding_types)/2 + 0.5)\n",
        "            plt.bar(x + offset, values, width, label=emb_type.upper())\n",
        "\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.title(f'{metric.capitalize()} Comparison: CBOW vs Skip-gram vs TF-IDF vs SBERT')\n",
        "        plt.xticks(x, model_names)\n",
        "        plt.ylim(0, 1)\n",
        "        plt.legend()\n",
        "        plt.grid(True, axis='y')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(f\"{metric}_comparison.png\")\n",
        "        plt.close()\n",
        "\n",
        "def plot_metrics_comparison(results_dict):\n",
        "    \"\"\"Plot performance metrics comparison between models and embedding types for both train and test sets\"\"\"\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    \n",
        "    # For training set\n",
        "    plot_set_comparison(results_dict, 'train', metrics, \"Training Set\")\n",
        "    \n",
        "    # For test set\n",
        "    plot_set_comparison(results_dict, 'test', metrics, \"Test Set\")\n",
        "\n",
        "\n",
        "def plot_set_comparison(results_dict, set_type, metrics, title_prefix):\n",
        "    \"\"\"Helper function to plot comparisons for a specific set (train or test)\"\"\"\n",
        "    # Prepare data for plotting\n",
        "    data = []\n",
        "    for embedding_type, models_data in results_dict.items():\n",
        "        for model_name, metrics_dict in models_data.items():\n",
        "            model_label = f\"{model_name} ({embedding_type.upper()})\"\n",
        "            for metric in metrics:\n",
        "                data.append({\n",
        "                    'Model': model_label,\n",
        "                    'Metric': metric.capitalize(),\n",
        "                    'Value': metrics_dict[set_type][metric]\n",
        "                })\n",
        "\n",
        "    df_plot = pd.DataFrame(data)\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    sns.barplot(x='Model', y='Value', hue='Metric', data=df_plot)\n",
        "    plt.title(f'{title_prefix} Performance Metrics: CBOW vs Skip-gram vs TF-IDF vs SBERT')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{set_type}_embedding_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_embedding_comparison(results_dict):\n",
        "    \"\"\"Plot comparison of embedding types across metrics and models for both train and test sets\"\"\"\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    model_names = list(next(iter(results_dict.values())).keys())\n",
        "    \n",
        "    # Plot for training set\n",
        "    plot_embedding_set_comparison(results_dict, model_names, metrics, 'train', \"Training Set\")\n",
        "    \n",
        "    # Plot for test set\n",
        "    plot_embedding_set_comparison(results_dict, model_names, metrics, 'test', \"Test Set\")\n",
        "\n",
        "\n",
        "def plot_embedding_set_comparison(results_dict, model_names, metrics, set_type, title_prefix):\n",
        "    \"\"\"Helper function to plot embedding comparison for a specific set (train or test)\"\"\"\n",
        "    # For each metric, compare embeddings across models\n",
        "    for metric in metrics:\n",
        "        plt.figure(figsize=(12, 7))\n",
        "\n",
        "        # Prepare data\n",
        "        x = np.arange(len(model_names))\n",
        "        width = 0.2  # Width of the bars\n",
        "\n",
        "        # Create the bars for each embedding type\n",
        "        embedding_types = list(results_dict.keys())\n",
        "        for i, emb_type in enumerate(embedding_types):\n",
        "            values = [results_dict[emb_type][model][set_type][metric] for model in model_names]\n",
        "            offset = width * (i - len(embedding_types)/2 + 0.5)\n",
        "            plt.bar(x + offset, values, width, label=emb_type.upper())\n",
        "\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.title(f'{title_prefix} {metric.capitalize()} Comparison: CBOW vs Skip-gram vs TF-IDF vs SBERT')\n",
        "        plt.xticks(x, model_names)\n",
        "        plt.ylim(0, 1)\n",
        "        plt.legend()\n",
        "        plt.grid(True, axis='y')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(f\"{set_type}_{metric}_comparison.png\")\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def plot_train_test_comparison(results_dict):\n",
        "    \"\"\"Plot comparison between training and test metrics for each model and embedding type\"\"\"\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    \n",
        "    for metric in metrics:\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        \n",
        "        # Count number of model-embedding combinations for x-axis positioning\n",
        "        all_models = []\n",
        "        for embedding_type, models in results_dict.items():\n",
        "            for model_name in models:\n",
        "                all_models.append(f\"{model_name} ({embedding_type.upper()})\")\n",
        "        \n",
        "        x = np.arange(len(all_models))\n",
        "        width = 0.35  # Width of the bars\n",
        "        \n",
        "        # Values for train and test\n",
        "        train_values = []\n",
        "        test_values = []\n",
        "        \n",
        "        # Collect values\n",
        "        for embedding_type, models in results_dict.items():\n",
        "            for model_name, metrics_dict in models.items():\n",
        "                train_values.append(metrics_dict['train'][metric])\n",
        "                test_values.append(metrics_dict['test'][metric])\n",
        "        \n",
        "        # Create bars\n",
        "        plt.bar(x - width/2, train_values, width, label='Training')\n",
        "        plt.bar(x + width/2, test_values, width, label='Test')\n",
        "        \n",
        "        plt.xlabel('Model and Embedding')\n",
        "        plt.ylabel(f'{metric.capitalize()} Score')\n",
        "        plt.title(f'Training vs Test {metric.capitalize()} Comparison')\n",
        "        plt.xticks(x, all_models, rotation=45, ha='right')\n",
        "        plt.ylim(0, 1)\n",
        "        plt.legend()\n",
        "        plt.grid(True, axis='y')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        plt.savefig(f\"train_test_{metric}_comparison.png\")\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def calculate_overfitting_metrics(results_dict):\n",
        "    \"\"\"Calculate and visualize overfitting metrics\"\"\"\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    \n",
        "    # Calculate differences between train and test for each metric\n",
        "    diff_data = []\n",
        "    \n",
        "    for embedding_type, models in results_dict.items():\n",
        "        for model_name, metrics_dict in models.items():\n",
        "            model_label = f\"{model_name} ({embedding_type.upper()})\"\n",
        "            \n",
        "            for metric in metrics:\n",
        "                train_value = metrics_dict['train'][metric]\n",
        "                test_value = metrics_dict['test'][metric] \n",
        "                diff = train_value - test_value\n",
        "                \n",
        "                diff_data.append({\n",
        "                    'Model': model_label,\n",
        "                    'Metric': metric.capitalize(),\n",
        "                    'Difference': diff\n",
        "                })\n",
        "    \n",
        "    df_diff = pd.DataFrame(diff_data)\n",
        "    \n",
        "    # Create a heatmap of the differences\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    pivot_df = df_diff.pivot(index='Model', columns='Metric', values='Difference')\n",
        "    sns.heatmap(pivot_df, annot=True, cmap='coolwarm', center=0, fmt='.3f')\n",
        "    plt.title('Overfitting Analysis: Difference Between Training and Test Metrics')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"overfitting_heatmap.png\")\n",
        "    plt.close()\n",
        "    \n",
        "    # Return the dataframe for further analysis\n",
        "    return df_diff\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the classification pipeline for all embedding types\"\"\"\n",
        "    logging.info(\"Starting embedding classification comparing CBOW, Skip-gram, TF-IDF and SBERT\")\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    # Process CBOW embeddings\n",
        "    all_results['cbow'] = process_embedding_type('cbow')\n",
        "\n",
        "    # Process Skip-gram embeddings\n",
        "    all_results['skipgram'] = process_embedding_type('skipgram')\n",
        "\n",
        "    # Process TF-IDF embeddings\n",
        "    all_results['tfidf'] = process_embedding_type('tfidf')\n",
        "\n",
        "    # Process SBERT embeddings\n",
        "    all_results['sbert'] = process_embedding_type('sbert')\n",
        "\n",
        "    # Compare models and embeddings\n",
        "    plot_metrics_comparison(all_results)\n",
        "    plot_embedding_comparison(all_results)\n",
        "    plot_train_test_comparison(all_results)\n",
        "    overfitting_df = calculate_overfitting_metrics(all_results)\n",
        "\n",
        "    # Save overall results for both train and test\n",
        "    # Training set results\n",
        "    train_results_df = pd.DataFrame({\n",
        "        f\"{model}_{emb_type}_train\": metrics['train']\n",
        "        for emb_type, models in all_results.items()\n",
        "        for model, metrics in models.items()\n",
        "    })\n",
        "    train_results_df.to_csv(\"train_embedding_comparison.csv\")\n",
        "    \n",
        "    # Test set results\n",
        "    test_results_df = pd.DataFrame({\n",
        "        f\"{model}_{emb_type}_test\": metrics['test']\n",
        "        for emb_type, models in all_results.items()\n",
        "        for model, metrics in models.items()\n",
        "    })\n",
        "    test_results_df.to_csv(\"test_embedding_comparison.csv\")\n",
        "    \n",
        "    # Save overfitting analysis\n",
        "    overfitting_df.to_csv(\"overfitting_analysis.csv\", index=False)\n",
        "\n",
        "    # Create summary tables\n",
        "    # For training set\n",
        "    train_summary_data = []\n",
        "    for emb_type, models in all_results.items():\n",
        "        for model_name, metrics in models.items():\n",
        "            row = {\n",
        "                'Embedding': emb_type.upper(),\n",
        "                'Model': model_name,\n",
        "                'Set': 'Training'\n",
        "            }\n",
        "            row.update({k.capitalize(): f\"{v:.4f}\" for k, v in metrics['train'].items()})\n",
        "            train_summary_data.append(row)\n",
        "    \n",
        "    # For test set\n",
        "    test_summary_data = []\n",
        "    for emb_type, models in all_results.items():\n",
        "        for model_name, metrics in models.items():\n",
        "            row = {\n",
        "                'Embedding': emb_type.upper(),\n",
        "                'Model': model_name,\n",
        "                'Set': 'Test'\n",
        "            }\n",
        "            row.update({k.capitalize(): f\"{v:.4f}\" for k, v in metrics['test'].items()})\n",
        "            test_summary_data.append(row)\n",
        "    \n",
        "    # Combine and save\n",
        "    summary_df = pd.DataFrame(train_summary_data + test_summary_data)\n",
        "    summary_df.to_csv(\"embedding_classification_summary.csv\", index=False)\n",
        "    \n",
        "    print(\"\\nClassification results summary:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Calculate average performance per embedding type for both sets\n",
        "    print(\"\\nAverage performance by embedding type:\")\n",
        "    for set_type, set_name in [('train', 'Training'), ('test', 'Test')]:\n",
        "        print(f\"\\n{set_name} Set:\")\n",
        "        for emb_type, models in all_results.items():\n",
        "            avg_metrics = {}\n",
        "            for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "                avg_metrics[metric] = np.mean([models[model][set_type][metric] for model in models])\n",
        "            print(f\"{emb_type.upper()}: \" + \", \".join([f\"{k.capitalize()}: {v:.4f}\" for k, v in avg_metrics.items()]))\n",
        "\n",
        "    logging.info(\"Classification comparison completed! Results saved to 'results' directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
