{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4ee125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 00:10:41 - INFO - Starting embedding classification comparing CBOW, Skip-gram and SBERT\n",
      "2025-05-08 00:10:41 - INFO - \n",
      "----------------------------------------------------------------------\n",
      "Processing CBOW embeddings\n",
      "----------------------------------------------------------------------\n",
      "2025-05-08 00:10:41 - INFO - Loading CBOW embeddings...\n",
      "/tmp/ipykernel_4598/1042049140.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(f\"{file_name}\")\n",
      "2025-05-08 00:10:42 - INFO - Successfully loaded cbow embeddings from PT file: (50000, 100)\n",
      "2025-05-08 00:10:42 - INFO - Loaded cbow dataframe with shape: (50000, 102)\n",
      "2025-05-08 00:10:42 - INFO - Data shapes: X_train=(40000, 100), y_train=(40000, 11)\n",
      "2025-05-08 00:10:42 - INFO - Classes: ['Customer rating' 'CustomerService' 'Damage' 'Priceperformance' 'Product'\n",
      " 'ProductFactory' 'ProductFailure' 'ProductQuality' 'ProductSize'\n",
      " 'Shipping' 'Size']\n",
      "2025-05-08 00:10:42 - INFO - \n",
      "==================================================\n",
      "Training SimpleNN with CBOW\n",
      "==================================================\n",
      "2025-05-08 00:10:43 - INFO - Epoch [1/20], Train Loss: 0.4118, Train Acc: 0.1606, Val Loss: 0.3733, Val Acc: 0.1692\n",
      "2025-05-08 00:10:49 - INFO - Epoch [5/20], Train Loss: 0.3480, Train Acc: 0.2071, Val Loss: 0.3371, Val Acc: 0.2069\n",
      "2025-05-08 00:10:54 - INFO - Epoch [10/20], Train Loss: 0.3334, Train Acc: 0.2306, Val Loss: 0.3234, Val Acc: 0.2195\n",
      "2025-05-08 00:11:00 - INFO - Epoch [15/20], Train Loss: 0.3241, Train Acc: 0.2447, Val Loss: 0.3180, Val Acc: 0.2352\n",
      "2025-05-08 00:11:05 - INFO - Epoch [20/20], Train Loss: 0.3195, Train Acc: 0.2476, Val Loss: 0.3140, Val Acc: 0.2378\n",
      "/tmp/ipykernel_4598/1042049140.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
      "2025-05-08 00:11:05 - INFO - Results for SimpleNN with CBOW:\n",
      "2025-05-08 00:11:05 - INFO - Training Set Metrics:\n",
      "2025-05-08 00:11:05 - INFO - Accuracy: 0.2476\n",
      "2025-05-08 00:11:05 - INFO - Precision: 0.8781\n",
      "2025-05-08 00:11:05 - INFO - Recall: 0.8941\n",
      "2025-05-08 00:11:05 - INFO - F1: 0.8847\n",
      "2025-05-08 00:11:05 - INFO - Test Set Metrics:\n",
      "2025-05-08 00:11:05 - INFO - Accuracy: 0.2378\n",
      "2025-05-08 00:11:05 - INFO - Precision: 0.8702\n",
      "2025-05-08 00:11:05 - INFO - Recall: 0.8873\n",
      "2025-05-08 00:11:05 - INFO - F1: 0.8774\n",
      "2025-05-08 00:11:06 - INFO - \n",
      "==================================================\n",
      "Training DeepNN with CBOW\n",
      "==================================================\n",
      "2025-05-08 00:11:09 - INFO - Epoch [1/20], Train Loss: 0.4304, Train Acc: 0.1544, Val Loss: 0.3757, Val Acc: 0.1589\n",
      "2025-05-08 00:11:14 - INFO - Epoch [5/20], Train Loss: 0.3485, Train Acc: 0.2311, Val Loss: 0.3202, Val Acc: 0.2231\n",
      "2025-05-08 00:11:28 - INFO - Epoch [10/20], Train Loss: 0.3293, Train Acc: 0.2506, Val Loss: 0.3032, Val Acc: 0.2411\n",
      "2025-05-08 00:11:41 - INFO - Epoch [15/20], Train Loss: 0.3206, Train Acc: 0.2601, Val Loss: 0.3018, Val Acc: 0.2416\n",
      "2025-05-08 00:11:52 - INFO - Epoch [20/20], Train Loss: 0.3139, Train Acc: 0.2860, Val Loss: 0.2941, Val Acc: 0.2591\n",
      "/tmp/ipykernel_4598/1042049140.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
      "2025-05-08 00:11:52 - INFO - Results for DeepNN with CBOW:\n",
      "2025-05-08 00:11:52 - INFO - Training Set Metrics:\n",
      "2025-05-08 00:11:52 - INFO - Accuracy: 0.2822\n",
      "2025-05-08 00:11:52 - INFO - Precision: 0.9210\n",
      "2025-05-08 00:11:52 - INFO - Recall: 0.8564\n",
      "2025-05-08 00:11:52 - INFO - F1: 0.8849\n",
      "2025-05-08 00:11:52 - INFO - Test Set Metrics:\n",
      "2025-05-08 00:11:52 - INFO - Accuracy: 0.2589\n",
      "2025-05-08 00:11:52 - INFO - Precision: 0.9076\n",
      "2025-05-08 00:11:52 - INFO - Recall: 0.8476\n",
      "2025-05-08 00:11:52 - INFO - F1: 0.8741\n",
      "2025-05-08 00:11:53 - INFO - \n",
      "----------------------------------------------------------------------\n",
      "Processing SKIPGRAM embeddings\n",
      "----------------------------------------------------------------------\n",
      "2025-05-08 00:11:53 - INFO - Loading SKIPGRAM embeddings...\n",
      "/tmp/ipykernel_4598/1042049140.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(f\"{file_name}\")\n",
      "2025-05-08 00:11:53 - INFO - Successfully loaded skipgram embeddings from PT file: (50000, 100)\n",
      "2025-05-08 00:11:53 - INFO - Loaded skipgram dataframe with shape: (50000, 102)\n",
      "2025-05-08 00:11:53 - INFO - Data shapes: X_train=(40000, 100), y_train=(40000, 11)\n",
      "2025-05-08 00:11:53 - INFO - Classes: ['Customer rating' 'CustomerService' 'Damage' 'Priceperformance' 'Product'\n",
      " 'ProductFactory' 'ProductFailure' 'ProductQuality' 'ProductSize'\n",
      " 'Shipping' 'Size']\n",
      "2025-05-08 00:11:53 - INFO - \n",
      "==================================================\n",
      "Training SimpleNN with SKIPGRAM\n",
      "==================================================\n",
      "2025-05-08 00:11:54 - INFO - Epoch [1/20], Train Loss: 0.4426, Train Acc: 0.1287, Val Loss: 0.4055, Val Acc: 0.1289\n",
      "2025-05-08 00:12:01 - INFO - Epoch [5/20], Train Loss: 0.3775, Train Acc: 0.1749, Val Loss: 0.3724, Val Acc: 0.1712\n",
      "2025-05-08 00:12:08 - INFO - Epoch [10/20], Train Loss: 0.3635, Train Acc: 0.1858, Val Loss: 0.3607, Val Acc: 0.1757\n",
      "2025-05-08 00:12:14 - INFO - Epoch [15/20], Train Loss: 0.3563, Train Acc: 0.1961, Val Loss: 0.3542, Val Acc: 0.1865\n",
      "2025-05-08 00:12:19 - INFO - Epoch [20/20], Train Loss: 0.3513, Train Acc: 0.2064, Val Loss: 0.3507, Val Acc: 0.1927\n",
      "/tmp/ipykernel_4598/1042049140.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
      "2025-05-08 00:12:20 - INFO - Results for SimpleNN with SKIPGRAM:\n",
      "2025-05-08 00:12:20 - INFO - Training Set Metrics:\n",
      "2025-05-08 00:12:20 - INFO - Accuracy: 0.2064\n",
      "2025-05-08 00:12:20 - INFO - Precision: 0.8480\n",
      "2025-05-08 00:12:20 - INFO - Recall: 0.9003\n",
      "2025-05-08 00:12:20 - INFO - F1: 0.8722\n",
      "2025-05-08 00:12:20 - INFO - Test Set Metrics:\n",
      "2025-05-08 00:12:20 - INFO - Accuracy: 0.1927\n",
      "2025-05-08 00:12:20 - INFO - Precision: 0.8394\n",
      "2025-05-08 00:12:20 - INFO - Recall: 0.8928\n",
      "2025-05-08 00:12:20 - INFO - F1: 0.8643\n",
      "2025-05-08 00:12:20 - INFO - \n",
      "==================================================\n",
      "Training DeepNN with SKIPGRAM\n",
      "==================================================\n",
      "2025-05-08 00:12:23 - INFO - Epoch [1/20], Train Loss: 0.4553, Train Acc: 0.1139, Val Loss: 0.4099, Val Acc: 0.1141\n",
      "2025-05-08 00:12:38 - INFO - Epoch [5/20], Train Loss: 0.3769, Train Acc: 0.1776, Val Loss: 0.3548, Val Acc: 0.1704\n",
      "2025-05-08 00:12:52 - INFO - Epoch [10/20], Train Loss: 0.3602, Train Acc: 0.2040, Val Loss: 0.3415, Val Acc: 0.1858\n",
      "2025-05-08 00:13:06 - INFO - Epoch [15/20], Train Loss: 0.3501, Train Acc: 0.2236, Val Loss: 0.3344, Val Acc: 0.2018\n",
      "2025-05-08 00:13:22 - INFO - Epoch [20/20], Train Loss: 0.3438, Train Acc: 0.2264, Val Loss: 0.3325, Val Acc: 0.1975\n",
      "/tmp/ipykernel_4598/1042049140.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
      "2025-05-08 00:13:23 - INFO - Results for DeepNN with SKIPGRAM:\n",
      "2025-05-08 00:13:23 - INFO - Training Set Metrics:\n",
      "2025-05-08 00:13:23 - INFO - Accuracy: 0.2339\n",
      "2025-05-08 00:13:23 - INFO - Precision: 0.8808\n",
      "2025-05-08 00:13:23 - INFO - Recall: 0.8733\n",
      "2025-05-08 00:13:23 - INFO - F1: 0.8758\n",
      "2025-05-08 00:13:23 - INFO - Test Set Metrics:\n",
      "2025-05-08 00:13:23 - INFO - Accuracy: 0.2102\n",
      "2025-05-08 00:13:23 - INFO - Precision: 0.8667\n",
      "2025-05-08 00:13:23 - INFO - Recall: 0.8616\n",
      "2025-05-08 00:13:23 - INFO - F1: 0.8629\n",
      "2025-05-08 00:13:23 - INFO - \n",
      "----------------------------------------------------------------------\n",
      "Processing SBERT embeddings\n",
      "----------------------------------------------------------------------\n",
      "2025-05-08 00:13:23 - INFO - Loading SBERT embeddings...\n",
      "/tmp/ipykernel_4598/1042049140.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(f\"{file_name}\")\n",
      "2025-05-08 00:13:23 - INFO - Successfully loaded sbert embeddings from PT file: (200000, 384)\n",
      "2025-05-08 00:13:23 - INFO - Loaded sbert dataframe with shape: (200000, 386)\n",
      "2025-05-08 00:13:23 - INFO - Data shapes: X_train=(160000, 384), y_train=(160000, 11)\n",
      "2025-05-08 00:13:23 - INFO - Classes: ['Customer rating' 'CustomerService' 'Damage' 'Priceperformance' 'Product'\n",
      " 'ProductFactory' 'ProductFailure' 'ProductQuality' 'ProductSize'\n",
      " 'Shipping' 'Size']\n",
      "2025-05-08 00:13:24 - INFO - \n",
      "==================================================\n",
      "Training SimpleNN with SBERT\n",
      "==================================================\n",
      "2025-05-08 00:13:31 - INFO - Epoch [1/20], Train Loss: 0.2168, Train Acc: 0.6341, Val Loss: 0.1395, Val Acc: 0.6368\n",
      "2025-05-08 00:14:10 - INFO - Epoch [5/20], Train Loss: 0.1370, Train Acc: 0.6543, Val Loss: 0.1278, Val Acc: 0.6573\n",
      "2025-05-08 00:14:50 - INFO - Epoch [10/20], Train Loss: 0.1317, Train Acc: 0.6565, Val Loss: 0.1260, Val Acc: 0.6593\n",
      "2025-05-08 00:15:38 - INFO - Epoch [15/20], Train Loss: 0.1301, Train Acc: 0.6530, Val Loss: 0.1255, Val Acc: 0.6544\n",
      "2025-05-08 00:16:01 - INFO - Early stopping at epoch 17\n",
      "/tmp/ipykernel_4598/1042049140.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
      "2025-05-08 00:16:03 - INFO - Results for SimpleNN with SBERT:\n",
      "2025-05-08 00:16:03 - INFO - Training Set Metrics:\n",
      "2025-05-08 00:16:03 - INFO - Accuracy: 0.6576\n",
      "2025-05-08 00:16:03 - INFO - Precision: 0.9877\n",
      "2025-05-08 00:16:03 - INFO - Recall: 0.9290\n",
      "2025-05-08 00:16:03 - INFO - F1: 0.9566\n",
      "2025-05-08 00:16:03 - INFO - Test Set Metrics:\n",
      "2025-05-08 00:16:03 - INFO - Accuracy: 0.6606\n",
      "2025-05-08 00:16:03 - INFO - Precision: 0.9877\n",
      "2025-05-08 00:16:03 - INFO - Recall: 0.9291\n",
      "2025-05-08 00:16:03 - INFO - F1: 0.9567\n",
      "2025-05-08 00:16:03 - INFO - \n",
      "==================================================\n",
      "Training DeepNN with SBERT\n",
      "==================================================\n",
      "2025-05-08 00:16:15 - INFO - Epoch [1/20], Train Loss: 0.2676, Train Acc: 0.5299, Val Loss: 0.1704, Val Acc: 0.5314\n",
      "2025-05-08 00:17:20 - INFO - Epoch [5/20], Train Loss: 0.1534, Train Acc: 0.6492, Val Loss: 0.1354, Val Acc: 0.6512\n",
      "2025-05-08 00:18:03 - INFO - Early stopping at epoch 8\n",
      "/tmp/ipykernel_4598/1042049140.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
      "2025-05-08 00:18:05 - INFO - Results for DeepNN with SBERT:\n",
      "2025-05-08 00:18:05 - INFO - Training Set Metrics:\n",
      "2025-05-08 00:18:05 - INFO - Accuracy: 0.6492\n",
      "2025-05-08 00:18:05 - INFO - Precision: 0.9908\n",
      "2025-05-08 00:18:05 - INFO - Recall: 0.9226\n",
      "2025-05-08 00:18:05 - INFO - F1: 0.9542\n",
      "2025-05-08 00:18:05 - INFO - Test Set Metrics:\n",
      "2025-05-08 00:18:05 - INFO - Accuracy: 0.6512\n",
      "2025-05-08 00:18:05 - INFO - Precision: 0.9911\n",
      "2025-05-08 00:18:05 - INFO - Recall: 0.9223\n",
      "2025-05-08 00:18:05 - INFO - F1: 0.9541\n",
      "2025-05-08 00:18:06 - INFO - Classification comparison completed! Results saved to 'results' directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification results summary:\n",
      "Embedding    Model Train_Accuracy Train_Precision Train_Recall Train_F1 Test_Accuracy Test_Precision Test_Recall Test_F1\n",
      "     CBOW SimpleNN         0.2476          0.8781       0.8941   0.8847        0.2378         0.8702      0.8873  0.8774\n",
      "     CBOW   DeepNN         0.2822          0.9210       0.8564   0.8849        0.2589         0.9076      0.8476  0.8741\n",
      " SKIPGRAM SimpleNN         0.2064          0.8480       0.9003   0.8722        0.1927         0.8394      0.8928  0.8643\n",
      " SKIPGRAM   DeepNN         0.2339          0.8808       0.8733   0.8758        0.2102         0.8667      0.8616  0.8629\n",
      "    SBERT SimpleNN         0.6576          0.9877       0.9290   0.9566        0.6606         0.9877      0.9291  0.9567\n",
      "    SBERT   DeepNN         0.6492          0.9908       0.9226   0.9542        0.6512         0.9911      0.9223  0.9541\n",
      "\n",
      "Average performance by embedding type:\n",
      "\n",
      "TRAIN SET:\n",
      "CBOW: Accuracy: 0.2649, Precision: 0.8996, Recall: 0.8753, F1: 0.8848\n",
      "SKIPGRAM: Accuracy: 0.2201, Precision: 0.8644, Recall: 0.8868, F1: 0.8740\n",
      "SBERT: Accuracy: 0.6534, Precision: 0.9893, Recall: 0.9258, F1: 0.9554\n",
      "\n",
      "VAL SET:\n",
      "CBOW: Accuracy: 0.2484, Precision: 0.8889, Recall: 0.8675, F1: 0.8758\n",
      "SKIPGRAM: Accuracy: 0.2015, Precision: 0.8531, Recall: 0.8772, F1: 0.8636\n",
      "SBERT: Accuracy: 0.6559, Precision: 0.9894, Recall: 0.9257, F1: 0.9554\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# === 1. Helper Functions ===\n",
    "def safe_parse(lst_str):\n",
    "    \"\"\"Safely parse string representations of lists\"\"\"\n",
    "    try:\n",
    "        if isinstance(lst_str, list):\n",
    "            return lst_str\n",
    "        return ast.literal_eval(lst_str)\n",
    "    except:\n",
    "        return [lst_str]\n",
    "\n",
    "# === 2. Load Embeddings ===\n",
    "def load_embeddings(embedding_type):\n",
    "    \"\"\"Load document embeddings from either PT or CSV files\n",
    "\n",
    "    Args:\n",
    "        embedding_type: 'cbow', 'skipgram', 'tfidf', or 'sbert'\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading {embedding_type.upper()} embeddings...\")\n",
    "\n",
    "    try:\n",
    "        # First try to load from PT file\n",
    "        if embedding_type == 'tfidf':\n",
    "            file_name = \"tfidf_amazon.pt\"\n",
    "        elif embedding_type == 'sbert':\n",
    "            file_name = f\"{embedding_type}_output_amazon.pt\"\n",
    "        else:\n",
    "            file_name = f\"{embedding_type}_amazon.pt\"\n",
    "        data = torch.load(f\"{file_name}\")\n",
    "        embeddings = data['embeddings'].numpy()\n",
    "        labels = data['labels']\n",
    "        indices = data['indices']\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(embeddings)\n",
    "        df.insert(0, 'original_index', indices)\n",
    "        df['labels'] = labels\n",
    "        logging.info(f\"Successfully loaded {embedding_type} embeddings from PT file: {embeddings.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback to CSV file\n",
    "        logging.info(f\"Failed to load from PT file: {e}\")\n",
    "        logging.info(\"Trying to load from CSV file...\")\n",
    "\n",
    "        if embedding_type == 'tfidf':\n",
    "            file_name = \"tfidf_amazon.csv\"\n",
    "        elif embedding_type == \"sbert\":\n",
    "            return\n",
    "        else:\n",
    "            return\n",
    "            # file_name = f\"{embedding_type}_amazon.csv\"\n",
    "\n",
    "        csv_path = f\"{file_name}\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # For TF-IDF, check if we need to rename the label column\n",
    "        if embedding_type == 'tfidf' and 'CommentClass_en' in df.columns:\n",
    "            df.rename(columns={'CommentClass_en': 'labels'}, inplace=True)\n",
    "\n",
    "        logging.info(f\"Successfully loaded {embedding_type} embeddings from CSV: {csv_path}\")\n",
    "\n",
    "    # Ensure labels are in the correct format\n",
    "    if 'labels' in df.columns:\n",
    "        df['labels'] = df['labels'].apply(safe_parse)\n",
    "    else:\n",
    "        # Try to find alternative label column\n",
    "        label_candidates = ['CommentClass_en', 'label', 'classes', 'class']\n",
    "        for col in label_candidates:\n",
    "            if col in df.columns:\n",
    "                df.rename(columns={col: 'labels'}, inplace=True)\n",
    "                df['labels'] = df['labels'].apply(safe_parse)\n",
    "                break\n",
    "        else:\n",
    "            logging.error(f\"No label column found in {embedding_type} embeddings\")\n",
    "            raise ValueError(f\"No label column found in {embedding_type} embeddings\")\n",
    "    return df\n",
    "\n",
    "# === 3. Neural Network Models ===\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Simple Feed-Forward Neural Network for multi-label classification\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    \"\"\"Deep Neural Network with multiple hidden layers\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.4):\n",
    "        super(DeepNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        # Add hidden layers\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = dim\n",
    "\n",
    "        # Add output layer with sigmoid for multi-label\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# === 4. Training and Evaluation ===\n",
    "\n",
    "def prepare_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Prepare data for training and evaluation\"\"\"\n",
    "    # Extract features and labels\n",
    "    feature_cols = [col for col in df.columns if col not in ['original_index', 'labels', 'Unnamed: 0']]\n",
    "    X = df[feature_cols].values\n",
    "\n",
    "    # Process labels\n",
    "    y_raw = df['labels'].tolist()\n",
    "\n",
    "    # Binarize labels\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(y_raw)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=None\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Data shapes: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "    logging.info(f\"Classes: {mlb.classes_}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, mlb.classes_\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, model_name, embedding_type,\n",
    "                batch_size=32, lr=0.001, num_epochs=20, patience=3):\n",
    "    \"\"\"Train and evaluate a PyTorch model\"\"\"\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # For early stopping\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        y_train_pred_all = []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Store predictions for accuracy calculation\n",
    "            y_train_pred_batch = (outputs > 0.5).float()\n",
    "            y_train_pred_all.append(y_train_pred_batch)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(X_train_tensor)\n",
    "            train_pred_binary = (train_outputs > 0.5).float().numpy()\n",
    "            train_accuracy = accuracy_score(y_train, train_pred_binary)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Validation\n",
    "            val_outputs = model(X_test_tensor)\n",
    "            val_loss = criterion(val_outputs, y_test_tensor)\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            val_pred_binary = (val_outputs > 0.5).float().numpy()\n",
    "            val_accuracy = accuracy_score(y_test, val_pred_binary)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), f\"best_{embedding_type}_{model_name}_classifier.pt\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            logging.info(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                       f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "                       f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "        # Check if early stopping criteria is met\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load(f\"best_{embedding_type}_{model_name}_classifier.pt\"))\n",
    "\n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train_tensor)\n",
    "        train_pred_binary = (train_outputs > 0.5).float().numpy()\n",
    "        \n",
    "        val_outputs = model(X_test_tensor)\n",
    "        val_pred_binary = (val_outputs > 0.5).float().numpy()\n",
    "\n",
    "    # Calculate metrics for training set\n",
    "    train_accuracy = accuracy_score(y_train, train_pred_binary)\n",
    "    train_precision = precision_score(y_train, train_pred_binary, average='weighted', zero_division=0)\n",
    "    train_recall = recall_score(y_train, train_pred_binary, average='weighted', zero_division=0)\n",
    "    train_f1 = f1_score(y_train, train_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate metrics for validation/test set\n",
    "    val_accuracy = accuracy_score(y_test, val_pred_binary)\n",
    "    val_precision = precision_score(y_test, val_pred_binary, average='weighted', zero_division=0)\n",
    "    val_recall = recall_score(y_test, val_pred_binary, average='weighted', zero_division=0)\n",
    "    val_f1 = f1_score(y_test, val_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Store metrics in dictionaries\n",
    "    train_metrics = {\n",
    "        'accuracy': train_accuracy,\n",
    "        'precision': train_precision,\n",
    "        'recall': train_recall,\n",
    "        'f1': train_f1,\n",
    "    }\n",
    "    \n",
    "    val_metrics = {\n",
    "        'accuracy': val_accuracy,\n",
    "        'precision': val_precision,\n",
    "        'recall': val_recall,\n",
    "        'f1': val_f1,\n",
    "    }\n",
    "\n",
    "    # Get per-class metrics for validation set\n",
    "    class_report = classification_report(y_test, val_pred_binary,\n",
    "                                        zero_division=0, output_dict=True)\n",
    "\n",
    "    return model, train_metrics, val_metrics, train_losses, val_losses, train_accuracies, val_accuracies, class_report\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, model_name, embedding_type):\n",
    "    \"\"\"Plot training and validation loss and accuracy curves\"\"\"\n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'Loss Curves for {model_name} ({embedding_type.upper()})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy Curves for {model_name} ({embedding_type.upper()})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{embedding_type}_{model_name}_training_curves.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_metrics_comparison(results_dict, set_type):\n",
    "    \"\"\"Plot performance metrics comparison between models and embedding types\"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for embedding_type, models_data in results_dict.items():\n",
    "        for model_name, sets_data in models_data.items():\n",
    "            metrics_dict = sets_data[set_type]\n",
    "            model_label = f\"{model_name} ({embedding_type.upper()})\"\n",
    "            for metric in metrics:\n",
    "                data.append({\n",
    "                    'Model': model_label,\n",
    "                    'Metric': metric.capitalize(),\n",
    "                    'Value': metrics_dict[metric]\n",
    "                })\n",
    "\n",
    "    df_plot = pd.DataFrame(data)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    sns.barplot(x='Model', y='Value', hue='Metric', data=df_plot)\n",
    "    plt.title(f'{set_type.capitalize()} Set Performance Metrics Comparison')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{set_type}_embedding_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_train_test_comparison(results_dict):\n",
    "    \"\"\"Plot comparison between training and test set performance for each model and embedding type\"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Prepare data\n",
    "        data = []\n",
    "        for embedding_type, models_data in results_dict.items():\n",
    "            for model_name, sets_data in models_data.items():\n",
    "                model_label = f\"{model_name} ({embedding_type.upper()})\"\n",
    "                train_value = sets_data['train'][metric]\n",
    "                test_value = sets_data['val'][metric]\n",
    "                \n",
    "                data.append({\n",
    "                    'Model': model_label,\n",
    "                    'Set': 'Train',\n",
    "                    'Value': train_value\n",
    "                })\n",
    "                data.append({\n",
    "                    'Model': model_label,\n",
    "                    'Set': 'Test',\n",
    "                    'Value': test_value\n",
    "                })\n",
    "        \n",
    "        df_plot = pd.DataFrame(data)\n",
    "        \n",
    "        # Create the plot\n",
    "        sns.barplot(x='Model', y='Value', hue='Set', data=df_plot)\n",
    "        plt.title(f'{metric.capitalize()} Comparison: Training vs Test Performance')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{metric}_train_test_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# === 5. Main Function ===\n",
    "\n",
    "def process_embedding_type(embedding_type):\n",
    "    \"\"\"Process a specific embedding type (CBOW, Skip-gram, or SBERT)\"\"\"\n",
    "    logging.info(f\"\\n{'-'*70}\\nProcessing {embedding_type.upper()} embeddings\\n{'-'*70}\")\n",
    "\n",
    "    # Load embeddings\n",
    "    df = load_embeddings(embedding_type)\n",
    "    logging.info(f\"Loaded {embedding_type} dataframe with shape: {df.shape}\")\n",
    "\n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test, classes = prepare_data(df)\n",
    "\n",
    "    # Model parameters\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "\n",
    "    # Define models to test\n",
    "    models = [\n",
    "        {\n",
    "            'name': 'SimpleNN',\n",
    "            'model': SimpleNN(input_dim, hidden_dim=128, output_dim=output_dim)\n",
    "        },\n",
    "        {\n",
    "            'name': 'DeepNN',\n",
    "            'model': DeepNN(\n",
    "                input_dim,\n",
    "                hidden_dims=[256, 128, 64],\n",
    "                output_dim=output_dim\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "\n",
    "    for model_info in models:\n",
    "        name = model_info['name']\n",
    "        model = model_info['model']\n",
    "\n",
    "        logging.info(f\"\\n{'='*50}\\nTraining {name} with {embedding_type.upper()}\\n{'='*50}\")\n",
    "\n",
    "        trained_model, train_metrics, val_metrics, train_losses, val_losses, train_accuracies, val_accuracies, class_report = train_model(\n",
    "            model, X_train, y_train, X_test, y_test, name, embedding_type\n",
    "        )\n",
    "\n",
    "        # Log results\n",
    "        logging.info(f\"Results for {name} with {embedding_type.upper()}:\")\n",
    "        logging.info(\"Training Set Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            logging.info(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "        \n",
    "        logging.info(\"Test Set Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            logging.info(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "        # Plot learning curves\n",
    "        plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, name, embedding_type)\n",
    "\n",
    "        # Save detailed class report\n",
    "        pd.DataFrame(class_report).transpose().to_csv(\n",
    "            f\"{embedding_type}_{name}_class_report.csv\"\n",
    "        )\n",
    "\n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'class_names': classes,\n",
    "            'train_metrics': train_metrics,\n",
    "            'val_metrics': val_metrics,\n",
    "            'architecture': str(trained_model)\n",
    "        }, f\"{embedding_type}_{name}_classifier.pt\")\n",
    "\n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'train': train_metrics,\n",
    "            'val': val_metrics\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the classification pipeline for all embedding types\"\"\"\n",
    "    logging.info(\"Starting embedding classification comparing CBOW, Skip-gram and SBERT\")\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    # Process CBOW embeddings\n",
    "    all_results['cbow'] = process_embedding_type('cbow')\n",
    "\n",
    "    # Process Skip-gram embeddings\n",
    "    all_results['skipgram'] = process_embedding_type('skipgram')\n",
    "\n",
    "    # Process SBERT embeddings\n",
    "    all_results['sbert'] = process_embedding_type('sbert')\n",
    "\n",
    "    # Compare models and embeddings for both training and test sets\n",
    "    plot_metrics_comparison(all_results, 'train')\n",
    "    plot_metrics_comparison(all_results, 'val')\n",
    "    \n",
    "    # Compare training vs test performance\n",
    "    plot_train_test_comparison(all_results)\n",
    "\n",
    "    # Save overall results\n",
    "    for set_type in ['train', 'val']:\n",
    "        results_df = pd.DataFrame({\n",
    "            f\"{model}_{emb_type}_{set_type}\": metrics\n",
    "            for emb_type, models in all_results.items()\n",
    "            for model, sets_data in models.items()\n",
    "            for set_name, metrics in sets_data.items()\n",
    "            if set_name == set_type\n",
    "        })\n",
    "        results_df.to_csv(f\"{set_type}_embedding_comparison.csv\")\n",
    "\n",
    "    # Create a summary table\n",
    "    summary_data = []\n",
    "    for emb_type, models in all_results.items():\n",
    "        for model_name, sets_data in models.items():\n",
    "            row = {\n",
    "                'Embedding': emb_type.upper(),\n",
    "                'Model': model_name\n",
    "            }\n",
    "            \n",
    "            # Add training metrics\n",
    "            for k, v in sets_data['train'].items():\n",
    "                row[f'Train_{k.capitalize()}'] = f\"{v:.4f}\"\n",
    "                \n",
    "            # Add test metrics\n",
    "            for k, v in sets_data['val'].items():\n",
    "                row[f'Test_{k.capitalize()}'] = f\"{v:.4f}\"\n",
    "                \n",
    "            summary_data.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(\"embedding_classification_summary.csv\", index=False)\n",
    "    print(\"\\nClassification results summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Calculate average performance per embedding type for both sets\n",
    "    print(\"\\nAverage performance by embedding type:\")\n",
    "    for set_type in ['train', 'val']:\n",
    "        print(f\"\\n{set_type.upper()} SET:\")\n",
    "        for emb_type, models in all_results.items():\n",
    "            avg_metrics = {}\n",
    "            for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "                avg_metrics[metric] = np.mean([models[model][set_type][metric] for model in models])\n",
    "            print(f\"{emb_type.upper()}: \" + \", \".join([f\"{k.capitalize()}: {v:.4f}\" for k, v in avg_metrics.items()]))\n",
    "\n",
    "    logging.info(\"Classification comparison completed! Results saved to 'results' directory.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
